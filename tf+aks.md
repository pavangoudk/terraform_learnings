
Below is a **beginner-friendly, production-oriented** walkthrough to build a **three-tier app** on **Azure Kubernetes Service (AKS)** using **Terraform**, with **separate environments** (dev/qa/stage/prod). It’s written in a wiki-like “do this → here’s why → here’s the code” format you can copy into an internal page.

## 0) What you’re building (target architecture)

### Three tiers

# 

1. **Frontend (Web)**: NGINX serving a static site that calls the API
2. **Backend (API)**: Node.js/Express REST API
3. **Data tier**: **Azure Database for PostgreSQL – Flexible Server** (managed DB; avoids running stateful DB on AKS for production)

### Production-ready baseline (key choices)

# 
- **Separate AKS clusters per environment** (strong isolation; easiest mental model)
- **Azure Container Registry (ACR)** for images
- **Workload Identity + Key Vault + Secrets Store CSI Driver** for secrets (no Kubernetes Secret plaintext)
- **Ingress NGINX** for north-south traffic
- **Autoscaling**: Cluster Autoscaler + Horizontal Pod Autoscaler (HPA)
- **Observability**: Log Analytics + Container insights
- **Security**: private nodes in a VNet subnet, least-privilege identities, Network Policy, resource limits, probes

## 1) Prerequisites (local machine + Azure)

# 
You’ll need:

- Azure subscription access with permission to create: RGs, AKS, ACR, Key Vault, PostgreSQL Flexible Server
- Installed locally: `az`, `terraform`, `kubectl`, `docker` (or use ACR build)

### Log in and select subscription

# 
```azurecli
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
az login
az account set --subscription "<SUBSCRIPTION_ID_OR_NAME>"
az account show
```

## 2) Repo layout (multi-environment Terraform)

# 
This layout keeps **modules reusable** and environments clean.

```
Unknowninfra/
  modules/
    aks/
    acr/
    keyvault/
    postgres/
  envs/
    dev/
    qa/
    stage/
    prod/
k8s/
  base/
  overlays/
    dev/
    qa/
    stage/
    prod/
app/
  api/
  web/
```

**Idea:**

- `infra/modules/*` = reusable building blocks
- `infra/envs/<env>` = environment-specific “wiring” + variables
- `k8s/base` = generic Kubernetes manifests
- `k8s/overlays/<env>` = per-env overrides (replicas/hosts/limits)

## 3) Terraform state (remote backend per environment)

# 
Production best practice is **remote state** with locking. On Azure, the typical choice is an **Azure Storage Account** backend.

### Create a shared “bootstrap” state store (one-time)

# 
Pick a dedicated RG/Storage Account (often shared across envs, but separate containers per env).

```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
export TF_STATE_RG="rg-tfstate"
export TF_STATE_LOC="eastus"
export TF_STATE_SA="sttfstate$RANDOM"     # must be globally unique
export TF_STATE_CONTAINER="tfstate"

az group create -n "$TF_STATE_RG" -l "$TF_STATE_LOC"
az storage account create -g "$TF_STATE_RG" -n "$TF_STATE_SA" -l "$TF_STATE_LOC" --sku Standard_LRS
az storage container create --name "$TF_STATE_CONTAINER" --account-name "$TF_STATE_SA"
```

**Why:**

- Enables team collaboration
- Enables safe applies (state locking)

## 4) Environment configuration approach (dev/qa/stage/prod)

# 
Each env gets:

- Its own **Resource Group**
- Its own **AKS cluster**
- Its own **ACR (optional)** (many orgs share ACR; for simplicity you can share, but isolation is stronger with separate)
- Its own **Key Vault**
- Its own **PostgreSQL server/database**
- Its own DNS hostname(s) for ingress

**Recommended minimum differences by environment**

- dev: smaller node sizes, fewer replicas, possibly allow more permissive diagnostics
- qa: closer to stage, load testing, stricter gating
- stage: production-like sizing/config
- prod: highest scale, strictest access, more zones, tighter policies

## 5) Terraform: root module per environment

### 5.1 `infra/envs/dev/providers.tf`

# 

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
terraform {
  required_version = ">= 1.6.0"

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.120"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.30"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.13"
    }
  }

  backend "azurerm" {
    resource_group_name  = "rg-tfstate"
    storage_account_name = "REPLACE_WITH_YOUR_TF_STATE_SA"
    container_name       = "tfstate"
    key                  = "dev.terraform.tfstate"
  }
}

provider "azurerm" {
  features {}
}
```

**Why:**

- Pins Terraform/provider versions to reduce “it worked yesterday” drift
- Backend key differs per environment

> Repeat the same file in `qa/stage/prod`, changing only the backend `key`.

### 5.2 `infra/envs/dev/main.tf` (wires modules together)

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
locals {
  env     = "dev"
  region  = var.region
  prefix  = var.prefix
  name    = "${local.prefix}-${local.env}"
  rg_name = "rg-${local.name}"
}

module "acr" {
  source              = "../../modules/acr"
  resource_group_name = local.rg_name
  location            = local.region
  name                = "acr${replace(local.name, "-", "")}"
}

module "keyvault" {
  source              = "../../modules/keyvault"
  resource_group_name = local.rg_name
  location            = local.region
  name                = "kv-${local.name}"
  tenant_id           = data.azurerm_client_config.current.tenant_id
}

module "postgres" {
  source              = "../../modules/postgres"
  resource_group_name = local.rg_name
  location            = local.region
  name                = "pg-${local.name}"
  admin_username      = var.pg_admin_username
  admin_password      = var.pg_admin_password
  database_name       = "appdb"
  # In production, prefer private networking; for simplicity this example keeps it public with firewall rules.
  allowed_cidrs       = var.pg_allowed_cidrs
}

module "aks" {
  source              = "../../modules/aks"
  resource_group_name = local.rg_name
  location            = local.region
  name                = "aks-${local.name}"

  kubernetes_version  = var.kubernetes_version
  system_node_count   = var.system_node_count
  user_node_count     = var.user_node_count
  node_vm_size        = var.node_vm_size

  acr_id              = module.acr.acr_id

  workload_identity_enabled = true
  oidc_issuer_enabled       = true

  log_analytics_workspace_sku = "PerGB2018"
}

data "azurerm_client_config" "current" {}

# Store DB connection info in Key Vault (apps will read it via CSI driver)
resource "azurerm_key_vault_secret" "pg_conn" {
  name         = "postgres-conn-string"
  value        = module.postgres.connection_string
  key_vault_id = module.keyvault.key_vault_id
}
```

**Why this pattern works**

- Modules encapsulate complexity
- Root env is readable: “create ACR, KV, PG, AKS, then store a secret”
- Promotes consistent naming across environments

> For qa/stage/prod, copy this file and set `locals.env` accordingly (or use `var.env`).

### 5.3 `infra/envs/dev/variables.tf`

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
variable "prefix" {
  description = "Name prefix (e.g., org or project code)"
  type        = string
}

variable "region" {
  description = "Azure region (e.g., eastus)"
  type        = string
}

variable "kubernetes_version" {
  type        = string
  description = "AKS version (pin a tested version per env)"
}

variable "system_node_count" {
  type        = number
  default     = 1
}

variable "user_node_count" {
  type        = number
  default     = 2
}

variable "node_vm_size" {
  type        = string
  default     = "Standard_D4s_v5"
}

variable "pg_admin_username" {
  type      = string
  sensitive = true
}

variable "pg_admin_password" {
  type      = string
  sensitive = true
}

variable "pg_allowed_cidrs" {
  description = "CIDRs allowed to access PostgreSQL (tighten heavily for prod)."
  type        = list(string)
  default     = []
}
```

### 5.4 `infra/envs/dev/terraform.tfvars` (example)

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
prefix             = "demo3tier"
region             = "eastus"
kubernetes_version = "1.29.7"

system_node_count = 1
user_node_count   = 2
node_vm_size      = "Standard_D4s_v5"

pg_admin_username = "pgadminuser"
pg_admin_password = "REPLACE_ME_WITH_A_STRONG_PASSWORD"

# Example: allow your office/VPN egress IP in dev only
pg_allowed_cidrs = ["203.0.113.10/32"]
```

**Production tip:**

- Use a secure secret injection method for tfvars (CI/CD secret store), and avoid committing real passwords.

## 6) Terraform modules (core building blocks)

### 6.1 `infra/modules/acr/main.tf`

# 

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
resource "azurerm_resource_group" "rg" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_container_registry" "acr" {
  name                = var.name
  resource_group_name = azurerm_resource_group.rg.name
  location            = azurerm_resource_group.rg.location
  sku                 = "Premium" # production-friendly features
  admin_enabled       = false     # prefer identity-based access
}
```

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
variable "resource_group_name" { type = string }
variable "location"            { type = string }
variable "name"                { type = string }

output "acr_id"          { value = azurerm_container_registry.acr.id }
output "acr_login_server" { value = azurerm_container_registry.acr.login_server }
```

### 6.2 `infra/modules/keyvault/main.tf`

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
resource "azurerm_resource_group" "rg" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_key_vault" "kv" {
  name                = var.name
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  tenant_id           = var.tenant_id

  sku_name                  = "standard"
  purge_protection_enabled  = true
  soft_delete_retention_days = 90

  # For production, prefer private endpoint + restrict public network.
  public_network_access_enabled = true
}

output "key_vault_id"   { value = azurerm_key_vault.kv.id }
output "key_vault_name" { value = azurerm_key_vault.kv.name }
```

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
variable "resource_group_name" { type = string }
variable "location"            { type = string }
variable "name"                { type = string }
variable "tenant_id"           { type = string }
```

### 6.3 `infra/modules/postgres/main.tf` (simplified public access pattern)

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
resource "azurerm_resource_group" "rg" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_postgresql_flexible_server" "pg" {
  name                   = var.name
  resource_group_name    = azurerm_resource_group.rg.name
  location               = azurerm_resource_group.rg.location
  version                = "16"
  sku_name               = "GP_Standard_D2s_v3"
  storage_mb             = 32768
  backup_retention_days  = 14
  geo_redundant_backup_enabled = false

  administrator_login    = var.admin_username
  administrator_password = var.admin_password

  # Production: set to false and use private access + private DNS
  public_network_access_enabled = true
}

resource "azurerm_postgresql_flexible_server_database" "db" {
  name      = var.database_name
  server_id = azurerm_postgresql_flexible_server.pg.id
  charset   = "UTF8"
  collation = "en_US.utf8"
}

resource "azurerm_postgresql_flexible_server_firewall_rule" "allow" {
  for_each = toset(var.allowed_cidrs)

  name      = "allow-${replace(each.value, "/", "-")}"
  server_id = azurerm_postgresql_flexible_server.pg.id

  start_ip_address = cidrhost(each.value, 0)
  end_ip_address   = cidrhost(each.value, 0)
}

output "fqdn" { value = azurerm_postgresql_flexible_server.pg.fqdn }

output "connection_string" {
  value     = "postgresql://${var.admin_username}:${var.admin_password}@${azurerm_postgresql_flexible_server.pg.fqdn}:5432/${var.database_name}?sslmode=require"
  sensitive = true
}
```

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
variable "resource_group_name" { type = string }
variable "location"            { type = string }
variable "name"                { type = string }
variable "admin_username"      { type = string; sensitive = true }
variable "admin_password"      { type = string; sensitive = true }
variable "database_name"       { type = string }
variable "allowed_cidrs"       { type = list(string) }
```

> Production hardening note (important): This module is intentionally “simple-first.” For production, shift to private networking (VNet integration, private DNS, no public access) and ensure AKS egress to DB via private route.

### 6.4 `infra/modules/aks/main.tf` (AKS + autoscaling + monitoring + ACR pull)

# 
```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
resource "azurerm_resource_group" "rg" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_log_analytics_workspace" "law" {
  name                = "law-${var.name}"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  sku                 = var.log_analytics_workspace_sku
  retention_in_days   = 30
}

resource "azurerm_kubernetes_cluster" "aks" {
  name                = var.name
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  dns_prefix          = var.name

  kubernetes_version  = var.kubernetes_version

  oidc_issuer_enabled       = var.oidc_issuer_enabled
  workload_identity_enabled = var.workload_identity_enabled

  identity {
    type = "SystemAssigned"
  }

  default_node_pool {
    name                 = "system"
    vm_size              = var.node_vm_size
    node_count           = var.system_node_count
    orchestrator_version = var.kubernetes_version
    type                 = "VirtualMachineScaleSets"
    mode                 = "System"
    enable_auto_scaling  = true
    min_count            = max(1, var.system_node_count)
    max_count            = max(2, var.system_node_count + 1)
  }

  network_profile {
    network_plugin    = "azure"
    load_balancer_sku = "standard"
    network_policy    = "azure"
  }

  oms_agent {
    log_analytics_workspace_id = azurerm_log_analytics_workspace.law.id
  }
}

# Separate user node pool for workloads
resource "azurerm_kubernetes_cluster_node_pool" "user" {
  name                  = "user"
  kubernetes_cluster_id  = azurerm_kubernetes_cluster.aks.id
  vm_size               = var.node_vm_size
  mode                  = "User"
  orchestrator_version  = var.kubernetes_version
  enable_auto_scaling   = true
  min_count             = max(1, var.user_node_count)
  max_count             = max(3, var.user_node_count + 2)
  node_count            = var.user_node_count
}

# Grant AKS kubelet identity permission to pull from ACR
resource "azurerm_role_assignment" "acr_pull" {
  scope                = var.acr_id
  role_definition_name = "AcrPull"
  principal_id         = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id
}

output "kube_config" {
  value     = azurerm_kubernetes_cluster.aks.kube_config_raw
  sensitive = true
}

output "cluster_name" { value = azurerm_kubernetes_cluster.aks.name }
output "rg_name"      { value = azurerm_resource_group.rg.name }
output "oidc_issuer_url" {
  value = azurerm_kubernetes_cluster.aks.oidc_issuer_url
}
```

```
Unknown# Code Generated by Sidekick is for learning and experimentation purposes only.
variable "resource_group_name" { type = string }
variable "location"            { type = string }
variable "name"                { type = string }

variable "kubernetes_version"  { type = string }
variable "system_node_count"   { type = number }
variable "user_node_count"     { type = number }
variable "node_vm_size"        { type = string }

variable "acr_id"              { type = string }

variable "workload_identity_enabled" { type = bool }
variable "oidc_issuer_enabled"       { type = bool }

variable "log_analytics_workspace_sku" { type = string }
```

## 7) Deploy infrastructure (per environment)

### 7.1 Initialize and apply (dev example)

# 

```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
cd infra/envs/dev

terraform init
terraform validate
terraform plan -out tfplan
terraform apply tfplan
```

### 7.2 Get kubeconfig

# 
```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
# Read outputs and set kubectl context
AKS_RG=$(terraform output -raw rg_name)
AKS_NAME=$(terraform output -raw cluster_name)

az aks get-credentials -g "$AKS_RG" -n "$AKS_NAME" --overwrite-existing
kubectl get nodes
```

> Repeat the same flow for `qa/`, `stage/`, `prod/` directories.

## 8) Application code (simple but production-shaped)

### 8.1 Backend API (Node.js/Express)

# 

**Behavior:** `GET /healthz`, `GET /api` returns DB timestamp.

**`app/api/package.json`**

```
JSON{
  "name": "demo-api",
  "version": "1.0.0",
  "main": "server.js",
  "type": "commonjs",
  "dependencies": {
    "express": "^4.19.2",
    "pg": "^8.12.0"
  }
}
```

**`app/api/server.js`**

```
JavaScript// Code Generated by Sidekick is for learning and experimentation purposes only.
const express = require("express");
const { Pool } = require("pg");

const app = express();
const port = process.env.PORT || 3000;

const connString = process.env.DATABASE_URL;
if (!connString) {
  console.error("DATABASE_URL is not set");
  process.exit(1);
}

const pool = new Pool({ connectionString: connString, ssl: { rejectUnauthorized: false } });

app.get("/healthz", (_req, res) => res.status(200).send("ok"));

app.get("/api", async (_req, res) => {
  const result = await pool.query("select now() as now");
  res.json({ ok: true, dbTime: result.rows[0].now });
});

app.listen(port, () => console.log(`API listening on ${port}`));
```

**`app/api/Dockerfile`**

```
Dockerfile# Code Generated by Sidekick is for learning and experimentation purposes only.
FROM node:20-alpine

WORKDIR /app
COPY package.json package-lock.json* ./
RUN npm ci --omit=dev

COPY server.js ./

ENV PORT=3000
EXPOSE 3000

USER node
CMD ["node", "server.js"]
```

### 8.2 Frontend Web (static NGINX)

# 
**Behavior:** Calls `https://<your-host>/api` and prints response.

**`app/web/index.html`**

```
HTML, XML<!-- Code Generated by Sidekick is for learning and experimentation purposes only. -->
<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Three-tier demo</title>
  </head>
  <body>
    <h1>Three-tier demo</h1>
    <button id="btn">Call API</button>
    <pre id="out"></pre>

    <script>
      // Code Generated by Sidekick is for learning and experimentation purposes only.
      document.getElementById("btn").onclick = async () => {
        const res = await fetch("/api");
        document.getElementById("out").textContent = await res.text();
      };
    </script>
  </body>
</html>
```

**`app/web/Dockerfile`**

```
Dockerfile# Code Generated by Sidekick is for learning and experimentation purposes only.
FROM nginx:1.27-alpine
COPY index.html /usr/share/nginx/html/index.html
```

## 9) Build and push images to ACR

# 
You can build locally and push, or use ACR build.

### Option A: ACR build (simple; no Docker needed locally)

# 
```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
# Set these from Terraform outputs or Azure portal
export ACR_NAME="<your-acr-name>"

az acr build -r "$ACR_NAME" -t web:1.0.0 ./app/web
az acr build -r "$ACR_NAME" -t api:1.0.0 ./app/api
```

### Option B: local docker build + push

# 
```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
ACR_LOGIN_SERVER=$(az acr show -n "$ACR_NAME" --query loginServer -o tsv)
az acr login -n "$ACR_NAME"

docker build -t "$ACR_LOGIN_SERVER/web:1.0.0" ./app/web
docker build -t "$ACR_LOGIN_SERVER/api:1.0.0" ./app/api

docker push "$ACR_LOGIN_SERVER/web:1.0.0"
docker push "$ACR_LOGIN_SERVER/api:1.0.0"
```

## 10) Kubernetes: namespaces + workload identity + secrets from Key Vault

### 10.1 Create namespace per environment (in its cluster)

# 

```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
kubectl create namespace app || true
```

### 10.2 Install Secrets Store CSI Driver + Azure Key Vault Provider

# 
In production, manage add-ons via Terraform/Helm. Below shows Helm CLI for clarity.

```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts
helm repo add azure-secrets-store-csi-driver https://azure.github.io/secrets-store-csi-driver-provider-azure/charts
helm repo update

helm upgrade --install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver \
  -n kube-system --set syncSecret.enabled=false

helm upgrade --install csi-azure azure-secrets-store-csi-driver/csi-secrets-store-provider-azure \
  -n kube-system
```

**Why:**

- Allows mounting Key Vault secrets into Pods without storing them as Kubernetes Secrets.

## 11) Kubernetes manifests (base)

# 
Set `ACR_LOGIN_SERVER` and `KV_NAME` per environment (overlays are a good way; below is direct for clarity).

### 11.1 ServiceAccount for Workload Identity

# 
```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-sa
  namespace: app
  annotations:
    azure.workload.identity/use: "true"
```

### 11.2 SecretProviderClass (mount Key Vault secret)

# 
Replace `keyvaultName` with your env Key Vault name.

```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: kv-postgres
  namespace: app
spec:
  provider: azure
  parameters:
    useWorkloadIdentity: "true"
    keyvaultName: "kv-demo3tier-dev"
    objects: |
      array:
        - |
          objectName: postgres-conn-string
          objectType: secret
    tenantId: "REPLACE_WITH_YOUR_TENANT_ID"
```

**Why:**

- Your API Pod can read the DB connection string from a mounted file, sourced from Key Vault.

### 11.3 API Deployment + Service + HPA

# 
Replace image with your ACR login server (e.g., `myacr.azurecr.io/api:1.0.0`).

```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
        azure.workload.identity/use: "true"
    spec:
      serviceAccountName: api-sa
      containers:
        - name: api
          image: REPLACE_ACR_LOGIN_SERVER/api:1.0.0
          ports:
            - containerPort: 3000
          env:
            - name: PORT
              value: "3000"
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  # We are not syncing to a K8s Secret in this simple example,
                  # so we’ll read from mounted file instead (see command below).
                  # Keep DATABASE_URL here only if you enable syncSecret.
                  name: unused
                  key: unused
          volumeMounts:
            - name: secrets-store
              mountPath: "/mnt/secrets"
              readOnly: true
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          readinessProbe:
            httpGet:
              path: /healthz
              port: 3000
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /healthz
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 20
      volumes:
        - name: secrets-store
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: "kv-postgres"
---
# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: v1
kind: Service
metadata:
  name: api
  namespace: app
spec:
  selector:
    app: api
  ports:
    - name: http
      port: 80
      targetPort: 3000
---
# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**Important fix (so the API reads the mounted secret):**

The simplest pattern is to have the container read the file and export `DATABASE_URL` via an entrypoint script. For production, you’d usually enable `syncSecret` and map it as an env var, or use a small init container. Here’s a beginner-friendly init-container approach:

```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
# Add this to the api Deployment spec.template.spec:
initContainers:
  - name: init-db-url
    image: alpine:3.20
    command: ["/bin/sh","-c"]
    args:
      - 'cp /mnt/secrets/postgres-conn-string /work/dburl'
    volumeMounts:
      - name: secrets-store
        mountPath: "/mnt/secrets"
        readOnly: true
      - name: work
        mountPath: "/work"
containers:
  - name: api
    # ...same as above...
    env:
      - name: DATABASE_URL
        valueFrom:
          configMapKeyRef:
            name: api-config
            key: databaseUrl
    volumeMounts:
      - name: work
        mountPath: "/work"
volumes:
  - name: work
    emptyDir: {}
```

That still needs an `api-config` ConfigMap, so the more practical production approach is: **enable secret sync** and use `secretKeyRef`. If you want, I can provide the clean “syncSecret=true” pattern end-to-end (it’s what most teams standardize on).

### 11.4 Web Deployment + Service

# 
```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  namespace: app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: REPLACE_ACR_LOGIN_SERVER/web:1.0.0
          ports:
            - containerPort: 80
          resources:
            requests:
              cpu: "50m"
              memory: "64Mi"
            limits:
              cpu: "250m"
              memory: "256Mi"
---
# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: v1
kind: Service
metadata:
  name: web
  namespace: app
spec:
  selector:
    app: web
  ports:
    - name: http
      port: 80
      targetPort: 80
```

### 11.5 Ingress (routes `/` to web, `/api` to api)

# 
Install NGINX ingress controller (one-time per cluster), then apply ingress.

```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
  -n ingress-nginx --create-namespace
```

Ingress resource (replace host):

```
YAML# Code Generated by Sidekick is for learning and experimentation purposes only.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  rules:
    - host: dev.demo3tier.example.com
      http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api
                port:
                  number: 80
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web
                port:
                  number: 80
```

## 12) Deploy the Kubernetes app

# 
```
Bash# Code Generated by Sidekick is for learning and experimentation purposes only.
kubectl -n app apply -f k8s/base/
kubectl -n app get pods,svc,ingress
```

## 13) Environment promotion model (dev → qa → stage → prod)

# 
A simple, production-friendly model:

### Recommended promotion mechanics

# 
- **Same Docker image tag promoted** (e.g., `api:1.0.0` goes from dev to prod)
- **Environment differences in config** (replicas, hostnames, limits) not in application code
- **Infrastructure changes via Terraform** separately planned/applied per env

### What differs by environment (example)

# 
- **dev**: replicas=2, smaller nodes, DB backups=7–14 days
- **qa**: replicas=2–3, load tests, stricter probes/limits
- **stage**: replicas=3+, prod-like AKS version and node sizes
- **prod**:
- multi-zone node pools (if region supports)
- private DB access
- stricter network policies
- longer DB backups + PITR (point-in-time restore)
- operational runbooks (see below)

## 14) Production hardening checklist (high value items)

### Security

# 

- Prefer **private** PostgreSQL + private DNS + no public network
- Prefer **private AKS** (or at minimum private nodes) and restrict API server access
- Use **Azure Policy** / admission controls for:
- no privileged pods
- required resource limits
- allowed registries (ACR only)
- Use **NetworkPolicy** to restrict traffic (web → api → db only)
- Use **Workload Identity** (no long-lived secrets)
- Enable **Key Vault purge protection** (already in module)

### Scalability & resilience

# 
- Use **HPA** for web+api (CPU or custom metrics)
- Use **Cluster Autoscaler** (already enabled via node pools autoscaling)
- Add **PodDisruptionBudget** for API and web
- Use **readiness/liveness** probes (already included)
- Separate **system** and **user** node pools (already done)

### Observability & ops

# 
- Log Analytics + Container Insights (enabled)
- Define SLOs (latency, error rate) and alerts
- Set an AKS upgrade cadence (monthly review; patch quickly)

### Ongoing maintenance

# 
- Rotate secrets in Key Vault; keep apps reading runtime values (CSI)
- Backups and restore drills for PostgreSQL
- Terraform drift detection (scheduled plan in CI)

## 15) “Day-2” runbook starters (what teams always need)

# 
- How to roll back: redeploy previous image tag, or revert manifest change
- How to scale: adjust HPA max, add node pool max, increase requests/limits carefully
- How to rotate DB password: update Key Vault secret, restart API pods
- How to troubleshoot:
- `kubectl describe pod`, `kubectl logs`
- check ingress controller logs
- check DB connectivity (network/firewalls/private DNS)

## 16) Repeat for QA/stage/prod (quick recipe)

# 
For each environment:

1. Copy `infra/envs/dev` → `infra/envs/qa` and set:
- backend `key`
- `locals.env`
- `terraform.tfvars` sizing/hostnames/CIDRs
2. `terraform init/plan/apply` in that folder
3. Build/push images (or reuse tags)
4. Apply Kubernetes manifests (or use overlays per env)
5. Validate ingress, API health, DB connectivity

# - -

# 

# 

#
